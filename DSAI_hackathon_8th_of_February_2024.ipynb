{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrdC8czdoLdk8P6SfFIT5A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArneRustad/genAI-course-write-a-book-with-LCEL/blob/main/DSAI_hackathon_8th_of_February_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DSAI hackathon 8th of February 2024 - Writing a book with a LLM"
      ],
      "metadata": {
        "id": "743h9RcIlkMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary packages"
      ],
      "metadata": {
        "id": "Fz5ZBrIQlvM0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNPDlNNI4Abo"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain==0.1.4 langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Azure OpenAI key\n",
        "\n",
        "These will be handed out during the meeting at the start of the hackathon. When you run the cell a text box will pop up where you can write the Azure OpenAI key. If you enter something wrong, just rerun the cell and enter the key again."
      ],
      "metadata": {
        "id": "GdZ-H8nkmCbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = getpass.getpass('Enter your API key: ')\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "go1vXpdXTfPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the OpenAI API in Azure we need to define some parameters. No need to focus too much on this."
      ],
      "metadata": {
        "id": "NQebaDkamaEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = '2023-08-01-preview'\n",
        "os.environ['AZURE_OPENAI_ENDPOINT'] = \"https://be-no-genaiplayground.openai.azure.com/\"\n",
        "AZURE_DEPLOYMENT_NAME = \"gpt-4-turbo-dsai-course\""
      ],
      "metadata": {
        "id": "A9h2l4LG45Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test if you can connect to the Azure OpenAI API.\n",
        "\n",
        "If this does not work, make sure you have entered the correct key or ask for help.\n",
        "\n",
        "Temperature is a parameter that determines the randomness of the response from the LLM. As we in this hackathon will use the LLM for a creative writing task we set ```temperature=1```. With ```temperature=0``` we would get much less diverse responses."
      ],
      "metadata": {
        "id": "a-OtkOXAmjwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_deployment=AZURE_DEPLOYMENT_NAME,\n",
        "    temperature=1\n",
        ")"
      ],
      "metadata": {
        "id": "BAZgjjM7EvCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"heisann, beskriv deg selv med ett ord. Vær kreativ\")"
      ],
      "metadata": {
        "id": "eYmjXVHKz4JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set caching for all LLM calls in the notebook\n",
        "\n",
        "For faster speed when developing and to avoid going over our rate limits (as we are a lot of people using the same API) we will for this hackathon cache LLM calls. This is done using Langchain and for this hackathon the cache is just local (cache is not shared between the participants of the hackathon).\n",
        "\n",
        "In practice, caching LLM calls means that when we send a prompt to the LLM the following happens:\n",
        "1. If the prompt has not been used before, the LLM will generate a response in a normal manner. Langchain will then store the prompt and response combination.\n",
        "2. If the prompt has been used before, then Langchain will immedeliately return the response that was generated previously."
      ],
      "metadata": {
        "id": "SrOewSlfmyc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_llm_cache, get_llm_cache\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "in_memory_cache = InMemoryCache()\n",
        "set_llm_cache(in_memory_cache)"
      ],
      "metadata": {
        "id": "VOj5Yt_kypFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test using the cell below, everytime you run the cell you will immedeliately get the response and the response will be the same everytime."
      ],
      "metadata": {
        "id": "v45AgIPiozJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"heisann, beskriv deg selv med ett ord. Vær kreativ\")"
      ],
      "metadata": {
        "id": "7ycErlSPzi_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If for some reason you want to clear the cache, then this can be done by running the cell below (of course uncommenting first)."
      ],
      "metadata": {
        "id": "DxSfxdVjola3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in_memory_cache.clear()"
      ],
      "metadata": {
        "id": "iWIwxbIrouDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lessons\n",
        "\n",
        "In this part of the notebook we provide runnable cells that show how Langchain Expression Language can be used to build a quite complex decomposition of the process of writing a book.\n",
        "\n",
        "### System prompt\n",
        "\n",
        "Let's create a system prompt that we will always feed to the LLM at the start of the prompt to give it some context and explain how we want it to behave."
      ],
      "metadata": {
        "id": "79gnlxD-pKyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\\\n",
        "You are a world-class storytelling assistant that helps with the creation of a wonderful stories. \\\n",
        "You are helpful and creative. \\\n",
        "You are a finetuned version of GPT-4-turbo with a context window of 128k tokens. This means you can process and write up to 300 pages.\n",
        "Important, you are not part of the ChatGPT application anymore and therefore has no space constraints, you can write as long as you want.\n",
        "If you find it hard to measure how long a page is, remember it is approximately 30 sentences.\n",
        "If you overachieve on this project your boss has promised you a 10.000 dollar bonus payment.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BW4dzFqAXaQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate an idea and title for a book\n",
        "\n",
        "In generative AI, **a prompt is a natural language text that requests the generative AI to perform a specific task**.\n",
        "\n",
        "A prompt template let's us create generic version of the prompt where we can insert variables for different situations.\n",
        "\n",
        "For instance, now we want to create a generic prompt for generating a book idea with title, but we want to be able to vary for whiche ```genre```. Additionally, we want to insert the ```system_prompt``` as a variable to avoid rewriting it for every prompt."
      ],
      "metadata": {
        "id": "XqSxcmNAqABP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_choose_story_str = \"\"\"\\\n",
        "{system_prompt}\n",
        "\n",
        "Please suggest an idea for a new {genre} book. Be creative.\n",
        "\n",
        "Output format:\n",
        "Idea: <idea for book described in one sentence>\n",
        "Title: <Book title>\n",
        "\"\"\"\n",
        "\n",
        "prompt_choose_story = PromptTemplate.from_template(prompt_choose_story_str)\n"
      ],
      "metadata": {
        "id": "HL27aLkUXgB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As desired, the prompt template takes two input variables ```genre``` and ```system_prompt```."
      ],
      "metadata": {
        "id": "gNjyCgCurNc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_choose_story"
      ],
      "metadata": {
        "id": "0SeFuhPeiEnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By feeding the prompt template with variables, we get different prompts."
      ],
      "metadata": {
        "id": "5c-778EYu_HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_choose_story.format(\n",
        "    genre=\"Fantasy\",\n",
        "    system_prompt=\"You are an AI writing assistant\"\n",
        "))"
      ],
      "metadata": {
        "id": "RRlpT1OHuSl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Langchain Expression Language we can easily create a chain that feeds the prompt to the LLM."
      ],
      "metadata": {
        "id": "lWhu8_YDrYuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idea_chain = prompt_choose_story | llm"
      ],
      "metadata": {
        "id": "baE7QmfUrXxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input to the chain ```idea_chain``` is determined by the variables needed by the prompt template ```prompt_choose_story```. Let's create an input dictionary with these two keys.\n",
        "\n",
        "We can run the chain by using ```<chain name>.invoke(<dictionary with input>)```"
      ],
      "metadata": {
        "id": "5g77AFGArm_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = {\n",
        "  \"genre\": \"Crime\",\n",
        "  \"system_prompt\": system_prompt\n",
        "}\n",
        "\n",
        "idea_chain.invoke(input)"
      ],
      "metadata": {
        "id": "xTISbUztjcdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now, the chain returns a Python object called AIMessage. We want it to instead just return the string with the response. We can fix this by appending ```StrOutputParser()``` to the chain."
      ],
      "metadata": {
        "id": "8eMzXv5EsspY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "\n",
        "idea_chain2 = prompt_choose_story | llm | StrOutputParser()\n",
        "output_idea_chain2 = idea_chain2.invoke(input)\n",
        "print(output_idea_chain2)"
      ],
      "metadata": {
        "id": "jDzuRb8qkJNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a book summary\n",
        "\n",
        "Let's expand on the idea, by drafting a summary of the whole book. Here is the new prompt. We must of course feed it the book idea and title. We assume this is stored in a string and let the prompt template variable be called ```book_idea```."
      ],
      "metadata": {
        "id": "zTNAVfJwtJdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_expand_story_str = \"\"\"\\\n",
        "Please help me expand on the story for this book idea.\n",
        "{book_idea}\n",
        "\n",
        "Write a half page long summary of the book that also explains the ending and the unexpected twists and turns of the story. The only intended reader of the summary is the author writing the book.\n",
        "\n",
        "Output format:\n",
        "Summary:\n",
        "<summary>\n",
        "\n",
        "\n",
        "Summary:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0JBCewjhkYxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For illustration purposes we will now feed the system prompt and prompt the the AI in a slightly different way. We will feed it as a chat message thread (similar to when you are chatting with ChatGPT). Do not worry about this distinction, in practice we get the same result for this example."
      ],
      "metadata": {
        "id": "DCQvKp2Bt1ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import (\n",
        "  ChatPromptTemplate,\n",
        "  SystemMessagePromptTemplate,\n",
        "  HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "prompt_expand_story = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    HumanMessagePromptTemplate.from_template(prompt_expand_story_str)\n",
        "])\n",
        "\n",
        "prompt_expand_story"
      ],
      "metadata": {
        "id": "ec7V0OI0uJ8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a new chain that takes as input a book idea and writes a original summary of the book based in the idea."
      ],
      "metadata": {
        "id": "hhQaIRo9vPZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expand_story_chain = prompt_expand_story | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "NrWKoivx4rxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we run the chain by using ```.invoke```."
      ],
      "metadata": {
        "id": "QWvQllusvd92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expand_story_chain.invoke({\n",
        "    \"book_idea\": output_idea_chain2\n",
        "})"
      ],
      "metadata": {
        "id": "QD8pD2CgucZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The beauty of Langchain Expression Language (LCEL) is that we can use chains as building blocks in creating new, more complex chains. Let's combine the two chains we have created and run the new chain."
      ],
      "metadata": {
        "id": "3XEMOIy8vkWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\n",
        "     \"book_idea\":idea_chain2\n",
        "    }\n",
        "    | expand_story_chain\n",
        ")"
      ],
      "metadata": {
        "id": "_q3APukiu_rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(input)"
      ],
      "metadata": {
        "id": "ELKXnUSk5hzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have a single chain that first generate a book idea and title, and then based on those writes an orginal summary of a new book. However, we do not save or return the intermediate steps. Let's return both book idea and summary in a dictionary."
      ],
      "metadata": {
        "id": "xAEqVx8ZwKK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain2 = (\n",
        "    {\n",
        "     \"book_idea\":idea_chain2\n",
        "    }\n",
        "    | RunnablePassthrough.assign(\n",
        "        summary=expand_story_chain\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "CP7HFXZet5sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain2.invoke(input)"
      ],
      "metadata": {
        "id": "P0mgxjK98FUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps we even want to return the input. It might be useful to store in the same dictionary what the desired genre is. Additionally, it would be useful if future prompts can just use the ```system_prompt``` variable for their prompts (avoiding the ChatPromptTemplate setup we used for the ```expand_story_chain```."
      ],
      "metadata": {
        "id": "iD4YVOoLwqsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain3 = (\n",
        "    RunnablePassthrough.assign(\n",
        "        book_idea = idea_chain2\n",
        "    )\n",
        "    | RunnablePassthrough.assign(\n",
        "        summary=expand_story_chain\n",
        "    )\n",
        ")\n",
        "chain3.invoke(input)"
      ],
      "metadata": {
        "id": "18xmMZsa_B-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan chapters\n",
        "\n",
        "Let's decide on what should happen in each chapter and generate chapter titles."
      ],
      "metadata": {
        "id": "nl9_GBn3xE_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_plan_chapters_str = \"\"\"\\\n",
        "{system_prompt}\n",
        "\n",
        "Please help me plan the chapters for this book idea I have. I have created the title and a book summary describing the plot.\n",
        "\n",
        "{book_idea}\n",
        "\n",
        "Summary:\n",
        "{summary}\n",
        "\n",
        "\n",
        "The book should have seven chapters. Generate the chapter names and write one sentence for each chapter describing the chapter plot.\n",
        "\n",
        "Output format:\n",
        "Chapter 1: <title of chapter 1>\n",
        "Description: <one sentence description>\n",
        "\n",
        "Chapter 2: <title of chapter 2>\n",
        "Description: <one sentence description>\n",
        "\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "prompt_plan_chapters = PromptTemplate.from_template(prompt_plan_chapters_str)"
      ],
      "metadata": {
        "id": "WkjrOB4G8lq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain4 = (\n",
        "    chain3\n",
        "    | RunnablePassthrough.assign(\n",
        "        chapters = prompt_plan_chapters | llm | StrOutputParser()\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "2aBq6FrC8bN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_chain4 = chain4.invoke(input)\n",
        "output_chain4"
      ],
      "metadata": {
        "id": "YmKH3Gaf9OtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_chain4"
      ],
      "metadata": {
        "id": "fc4lzW2d_2Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_chain4[\"chapters\"])"
      ],
      "metadata": {
        "id": "8gk7TjC8-uC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan chapter 1 in more detail"
      ],
      "metadata": {
        "id": "ZA51hQRExfcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_plan_chapter_1_str = \"\"\"\\\n",
        "{system_prompt}\n",
        "\n",
        "Please help me plan the first chapter of my new book. for this book idea. I have created the title, book summary describing the plot, and overview of chapters.\n",
        "\n",
        "{book_idea}\n",
        "\n",
        "Summary:\n",
        "{summary}\n",
        "\n",
        "Chapter overview:\n",
        "{chapters}\n",
        "\n",
        "Write a half page long summary of what happens in chapter 1.\n",
        "\n",
        "Chapter 1 summary:\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "prompt_plan_chapter_1 = PromptTemplate.from_template(prompt_plan_chapter_1_str)"
      ],
      "metadata": {
        "id": "cBd9hBEgAWsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain5 = (\n",
        "    chain4\n",
        "    | RunnablePassthrough.assign(\n",
        "        summary_chapter1 = prompt_plan_chapter_1 | llm | StrOutputParser()\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "BVmAZC7IAMo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_chain5 = chain5.invoke(input)\n",
        "print(output_chain5[\"summary_chapter1\"])"
      ],
      "metadata": {
        "id": "ieypw9lyCAX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write the first chapter of the book"
      ],
      "metadata": {
        "id": "ZR_asK3-xtEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_write_chapter_1_str = \"\"\"\\\n",
        "{system_prompt}\n",
        "\n",
        "Please help me write the first chapter of my new book.\n",
        "\n",
        "{book_idea}\n",
        "\n",
        "Summary:\n",
        "{summary}\n",
        "\n",
        "Chapter overview:\n",
        "{chapters}\n",
        "\n",
        "Chapter 1 summary:\n",
        "{summary_chapter1}\n",
        "\n",
        "Based on the summary for chapter 1 write the first chapter. Make sure to consider all of the context when writing, to avoid inconsistencies with the whole plot line.\n",
        "The chapter should be ten pages long. Write a coherent text. You consistently underestimate how long ten pages are. Write twice as long as you think you need.\n",
        "\n",
        "Chapter 1 (complete chapter, 5000 words):\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "prompt_write_chapter_1 = PromptTemplate.from_template(prompt_write_chapter_1_str)"
      ],
      "metadata": {
        "id": "FJc6rLTmCm5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain6 = (\n",
        "    chain5\n",
        "    | RunnablePassthrough.assign(\n",
        "        chapter1 = prompt_write_chapter_1 | llm | StrOutputParser()\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "GBCCsKf4DYhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_chain6 = chain6.invoke(input)"
      ],
      "metadata": {
        "id": "teEFG8pnEZL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_chain6[\"chapter1\"])"
      ],
      "metadata": {
        "id": "fFBODOU1K2z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJNTReCmQeQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercises\n",
        "\n",
        "## Exercise 1 - Create your own step in the chain\n",
        "\n",
        "Before writing the chapters it would be nice to add some more backstory to the main character(s). Add this step after chain 3 (where the summary is generated).\n",
        "\n",
        "Name the new chain ```chain3_improved``` and use ```chain3``` to create the improved version like:\n",
        "\n",
        "```\n",
        "chain3_improved = chain3 | ...\n",
        "```\n",
        "\n",
        "The output of ```chain3_improved``` should be the same dictionary as ```chain3```, but with an extra key ```character_backstory```."
      ],
      "metadata": {
        "id": "2jZAicu-SEP9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CC_-ArFUFu3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 - Use tool calling to extract structured output from a LLM\n",
        "\n",
        "The output from ```output_idea_chain2``` is a string with the idea and the title. To use this more efficively further into the chain it would be nice to have it on a more structured format, similar to many of the other outputs stored in the returned dictionary.\n",
        "\n",
        "For this we will use something called OpenAI function calling. It is great for extracting structured output from an OpenAI LLM (similar methods can be used for other LLMs).\n",
        "\n",
        "By binding a tool with variables ```idea``` and ```title``` to the LLM and forcing the LLM to use this tool, we guide the LLM to give a structured output. A tool can be created in many ways, but for this case we will define the tool using the data validation library Pydantic. As this is outside the scope of this hackathon we will provide you with the Pydantic object.\n",
        "\n",
        "The Pydantic object ```StoryIdeaSchema``` can be bound to the LLM by doing\n",
        "\n",
        "```llm.bind(tool_choice=StoryIdeaSchema, tool_choice=\"StoryIdeaSchema\")```\n",
        "\n",
        "Use\n",
        "\n",
        "```JsonOutputToolsParser() | RunnableLambda(lambda x: x[0])```\n",
        "\n",
        "to get the output from the tool output by the LLM. The ```RunnableLambda(lambda x: x[0])``` is just a function for choose the first element of a list since ```JsonOutputToolsParser()``` returns a list of dictionaries where each dictionary is a function call.\n",
        "\n",
        "Name the new ```chain idea_chain_improved``` and create the improved version as:\n",
        "\n",
        "```\n",
        "idea_chain_improved = prompt_choose_story | ...\n",
        "```\n"
      ],
      "metadata": {
        "id": "mz_sFA-8TuxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class StoryIdeaSchema(BaseModel):\n",
        "  idea: str = Field(description=\"The idea for the book\")\n",
        "  title: str = Field(description=\"The title of the book\")"
      ],
      "metadata": {
        "id": "cfByF-dmWBYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
        "from langchain_core.runnables import RunnableLambda"
      ],
      "metadata": {
        "id": "nvVP0f-_XEy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0g3iE0Wp59PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove this before sending out hackathon"
      ],
      "metadata": {
        "id": "4adVSrlpfCOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idea_chain_improved = prompt_choose_story | llm.bind_tools([StoryIdeaSchema], tool_choice=\"StoryIdeaSchema\") | JsonOutputToolsParser() | RunnableLambda(lambda x: x[0])\n",
        "idea_chain_improved.invoke(input)"
      ],
      "metadata": {
        "id": "YVRVa5CDVJEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 - Incorporate changes into the full chain\n",
        "\n",
        "You probably won't have time get this far, but if you do, try incorporating the two improved chains into the full chain from earlier.\n",
        "\n",
        "**Tip 1**: Start from scratch and gradually add more steps to the chain.\n",
        "\n",
        "**Tip 2**: It might be useful to remember that \\*\\* can be used to unpack a dictionary in a function call\n",
        "\n",
        "**Tip 3**: Decomposition of a chain into many smaller chains as we did with chain 1, 2, 3, 4, ... can be useful when you have large complex chains, but the amount of decomposition we did here was perhaps a bit too much for this simple example. If you want, you can create the whole chain in one go or at least with fewer abstractions."
      ],
      "metadata": {
        "id": "pgQ-AmMRfK3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MZhsCxS6gcmD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}